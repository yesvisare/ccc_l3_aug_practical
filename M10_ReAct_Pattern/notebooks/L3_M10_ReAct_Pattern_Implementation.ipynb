{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "---\n## Decision Card: ReAct Pattern (Quick Reference)\n\n**Save this for future reference when deciding between agent complexity and static pipeline simplicity.**\n\n### ‚úÖ BENEFIT\nEnables multi-step reasoning queries requiring 2-5 tools, solving complex questions like \"Compare our metrics to industry, calculate differences, and suggest strategies\" that static pipelines cannot handle. Adds autonomous tool selection without manual orchestration.\n\n### ‚ùå LIMITATION\nAdds 3-10s P95 latency compared to 300ms static pipeline. Agent reasoning is probabilistic‚Äî10-15% tool selection errors even with GPT-4. Infinite loops and state corruption require careful guard rails and monitoring to prevent production issues.\n\n### üí∞ COST\n- **Time to implement:** 40-60 hours (including testing, monitoring, debugging)\n- **Monthly cost at scale:** $1,500-15,000 for 100-1,000 req/hr (5-10x vs static)\n- **Complexity:** +500 lines code, LangChain dependency, state management infrastructure required\n\n### ü§î USE WHEN\n- You have <10% complex queries requiring 2-5 tools\n- Query volume <1,000/hr\n- Can tolerate 3-10s latency\n- Margin >$0.10/query supports agent cost\n- Queries genuinely need reasoning not just retrieval\n- Tools reliable >95% success rate\n\n### üö´ AVOID WHEN\n- 90%+ queries are simple retrieval (use static pipeline)\n- Need <1s P95 latency (use workflows)\n- Margin <$0.05/query or budget tight (use simpler alternatives)\n- Building first production system (use LangGraph managed framework)\n- Tools unreliable or have variable latency\n\n---\n\n## Next Steps\n\n1. **Test the implementation** - Run the cells above with your API key\n2. **Try the FastAPI server** - `python app.py` and test via `/query` endpoint\n3. **Run smoke tests** - `python tests_smoke.py` to verify everything works\n4. **Review common failures** - Study the 5 failure modes in the script\n5. **Practice** - Try the PractaThon challenges (Easy, Medium, Hard)\n\n### Resources\n- **Module script:** `augmented_M10_VideoM10_1_ReAct_Pat.md`\n- **Next module:** M10.2 - Building Custom Agent Tools & Integrations\n- **Discord:** #practathon channel for questions and feedback",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Section 5: Decision Framework - When to Use Agents vs Static Pipeline\n\nprint(\"=== DECISION FRAMEWORK ===\\n\")\n\n# Load failure scenarios from example data\nwith open('example_data.json', 'r') as f:\n    data = json.load(f)\n\nprint(\"Failure Scenarios to Watch For:\\n\")\nfor scenario_name, scenario_data in data.get('failure_scenarios', {}).items():\n    print(f\"‚Ä¢ {scenario_name.replace('_', ' ').title()}\")\n    print(f\"  Query: {scenario_data.get('query', 'N/A')}\")\n    if 'expected_behavior' in scenario_data:\n        print(f\"  Expected: {scenario_data['expected_behavior']}\")\n    print()\n\nprint(\"\\n=== COST ANALYSIS (from script) ===\\n\")\n\nscenarios = [\n    (\"Simple retrieval (90% traffic)\", \"$0.002\", \"Static pipeline\", \"300ms\"),\n    (\"Multi-step reasoning (<10% traffic)\", \"$0.01-0.03\", \"ReAct agent\", \"3-10s\"),\n]\n\nfor scenario, cost, approach, latency in scenarios:\n    print(f\"{scenario}\")\n    print(f\"  Approach: {approach}\")\n    print(f\"  Cost per query: {cost}\")\n    print(f\"  Latency: {latency}\")\n    print()\n\nprint(\"\\n=== MONTHLY COST AT SCALE ===\\n\")\nprint(\"| Scale | Compute | LLM Calls | Tool Costs | Total |\")\nprint(\"|-------|---------|-----------|------------|-------|\")\nprint(\"| Small (100/hr)  | $50   | $1,400  | $40    | **$1,490**   |\")\nprint(\"| Medium (1K/hr)  | $200  | $14,000 | $400   | **$14,600**  |\")\nprint(\"| Large (10K/hr)  | $800  | $140,000| $4,000 | **$144,800** |\")\n\nprint(\"\\n\\n=== KEY TAKEAWAY ===\")\nprint(\"Use ReAct agents for <10% of complex queries.\")\nprint(\"Keep static pipeline for 90%+ simple queries.\")\nprint(\"This gives you the best of both: flexibility + efficiency.\")\n\n# Expected:\n# - Decision framework clearly shows when to use agents vs static\n# - Cost breakdown helps with business case\n# - Failure scenarios warn about common pitfalls",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Section 5: Common Failures & Decision Framework\n\n### The 5 Common Agent Failures (from Script)\n\n| Failure | Symptom | Root Cause | Fix |\n|---------|---------|------------|-----|\n| **#1 Infinite Loop** | Agent repeats same action 3+ times | Tool returns unhelpful observation | Loop detection + better error messages |\n| **#2 Wrong Tool** | Uses RAG_Search when should use Calculator | Unclear tool descriptions or weak reasoning | Query classification + GPT-4 |\n| **#3 State Corruption** | Forgets previous conversation turns | No conversation memory | Session-based history |\n| **#4 Parsing Failure** | OutputParserException | Tool returns structured data (dict), not text | Standardize all tool outputs to strings |\n| **#5 No Stop Condition** | Keeps searching unnecessarily | Prompt doesn't emphasize efficiency | Add stopping criteria to prompt |\n\n### Real Failure Example from Script\n\n**Infinite Loop:**\n```\nQuery: \"What is the population of cities in California?\"\n\nStep 1 - Action: RAG_Search(\"California cities\")\nStep 1 - Observation: [No relevant documents found]\n\nStep 2 - Action: RAG_Search(\"California cities\")  # ‚Üê Same action!\nStep 2 - Observation: [No relevant documents found]\n\nStep 3 - Action: RAG_Search(\"California cities\")  # ‚Üê Same action again!\n... repeats until max_iterations reached\n```\n\n**Why this happens:** LLM doesn't understand that \"No documents found\" means \"this tool won't help, try different approach.\"\n\n### When NOT to Use ReAct Agents (Critical!)\n\n‚ùå **Don't use when:**\n- 90%+ queries are simple retrieval ‚Üí **use static pipeline**\n- Need <1s latency ‚Üí **use workflows or caching**\n- Margin <$0.05 per query ‚Üí **use cheaper alternatives**\n- Document corpus <100 docs ‚Üí **use static RAG**\n- Tools are unreliable (>10% failure rate) ‚Üí **use workflows with error handling**\n\n‚úÖ **Do use when:**\n- <10% queries but they're high-value (worth the cost)\n- Queries genuinely require 2+ tools\n- Can tolerate 3-10s latency\n- Margin >$0.10 per query supports $0.02 agent cost\n- Tools are reliable (>95% success rate)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Section 4: Test Queries (if agent available)\n\nimport time\n\n# Load example queries\nwith open('example_data.json', 'r') as f:\n    examples = json.load(f)\n\ntest_queries = [\n    (\"Simple RAG\", \"What is our refund policy?\"),\n    (\"Calculator\", \"What is 125000 * 1.15?\"),\n    (\"Industry Data\", \"What is the SaaS industry growth rate?\"),\n]\n\nprint(\"=== Query Tests ===\\n\")\n\nif agent is not None:\n    for i, (query_type, query) in enumerate(test_queries, 1):\n        print(f\"[Query {i}] {query_type}\")\n        print(f\"Question: {query}\")\n        \n        start = time.time()\n        result = agent.query(query)\n        duration = time.time() - start\n        \n        print(f\"Duration: {duration:.2f}s\")\n        print(f\"Steps: {result['num_steps']}\")\n        print(f\"Answer: {result['output'][:100]}...\")\n        if result['error']:\n            print(f\"Error: {result['error']}\")\n        print()\nelse:\n    print(\"‚ö†Ô∏è  Skipping query tests (agent not initialized)\")\n    print(\"   This is expected if OPENAI_API_KEY is not set\")\n    print()\n    print(\"   Example expected output:\")\n    print(\"   - Simple RAG: 1-2 steps, 2-4s\")\n    print(\"   - Calculator: 1-2 steps, 2-3s\")\n    print(\"   - Industry Data: 1-2 steps, 2-3s\")\n\n# Expected:\n# - If agent available: Each query completes in 2-5s with 1-2 reasoning steps\n# - If no agent: Graceful skip with example output description",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Section 4: Running Queries - Simple vs Complex\n\n### Query Types and Expected Tool Usage\n\n| Query Type | Example | Expected Tools | Steps |\n|------------|---------|----------------|-------|\n| **Simple RAG** | \"What is our refund policy?\" | RAG_Search | 1-2 |\n| **Calculation** | \"What is 125000 * 1.15?\" | Calculator | 1-2 |\n| **Benchmark** | \"What is SaaS industry growth rate?\" | Industry_Data | 1-2 |\n| **Multi-step** | \"Compare Q3 revenue to industry benchmark\" | RAG_Search ‚Üí Industry_Data ‚Üí Calculator | 4-5 |\n\n### What to Observe\n\nWhen you run a query, watch for:\n\n1. **Reasoning steps** - How many Thought ‚Üí Action ‚Üí Observation cycles?\n2. **Tool selection** - Does it pick the right tool first?\n3. **Stopping behavior** - Does it stop when it has enough info?\n4. **Fallback handling** - If agent fails, does it gracefully fall back?\n\n### Performance Expectations\n\n- **Simple queries:** 2-4 seconds, 1-2 steps\n- **Complex queries:** 5-10 seconds, 4-5 steps\n- **Agent failures:** <10% with fallback to static pipeline\n\n‚ö†Ô∏è **If API key is missing:** Queries will skip gracefully with a warning message.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Section 3: Initialize ReAct Agent (if API key available)\n\nimport os\n\n# Check if we can initialize the agent\nhas_api_key = bool(Config.OPENAI_API_KEY)\n\nprint(\"=== Agent Initialization ===\\n\")\nprint(f\"OpenAI API Key configured: {has_api_key}\")\nprint(f\"Agent enabled in config: {Config.ENABLE_AGENT}\")\nprint(f\"Model: {Config.AGENT_MODEL}\")\nprint(f\"Max iterations: {Config.AGENT_MAX_ITERATIONS}\")\nprint(f\"Timeout: {Config.AGENT_TIMEOUT_SECONDS}s\")\nprint()\n\nif has_api_key:\n    print(\"‚úì Initializing agent...\")\n    try:\n        agent = StatefulReActAgent(\n            model_name=Config.AGENT_MODEL,\n            temperature=Config.AGENT_TEMPERATURE,\n            max_iterations=Config.AGENT_MAX_ITERATIONS,\n            timeout_seconds=Config.AGENT_TIMEOUT_SECONDS\n        )\n        print(\"‚úì Agent initialized successfully!\")\n        print(f\"  Tools available: {len(agent.tools)}\")\n    except Exception as e:\n        print(f\"‚úó Agent initialization failed: {e}\")\n        agent = None\nelse:\n    print(\"‚ö†Ô∏è  Skipping agent initialization (no API key)\")\n    print(\"   Set OPENAI_API_KEY in .env to enable agent\")\n    agent = None\n\n# Expected:\n# - If API key present: Agent initializes with 3 tools\n# - If no API key: Graceful skip with warning message",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Section 3: The ReAct Loop - How Agents Think\n\n### The Core Pattern\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ User Query  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  THOUGHT: \"What do I need to do?\"   ‚îÇ ‚Üê Reasoning\n‚îÇ  Generated by LLM                   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  ACTION: Select and execute tool    ‚îÇ ‚Üê Acting\n‚îÇ  (e.g., search_docs, calculate)     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  OBSERVATION: Tool result           ‚îÇ ‚Üê Learning\n‚îÇ  (e.g., \"Found 3 docs about Q3\")    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ\n       ‚ñº\n    Repeat until Answer or Max Steps\n```\n\n### Key Components\n\n1. **Thought Generation:** LLM decides what to do next based on query + observation history\n2. **Action Selection:** Agent picks a tool from registry\n3. **Tool Execution:** Run the selected tool with extracted parameters\n4. **Observation Capture:** Store tool output for next reasoning step\n5. **Stopping Condition:** Agent decides \"I have enough info\" or hits max iterations\n\n### Why This Matters for Production\n\n‚úÖ **Handles 10x more query complexity** - Multi-step reasoning queries that were impossible before  \n‚úÖ **Reduces manual orchestration** - No need to code specific workflows for each query type  \n‚úÖ **Provides reasoning transparency** - You can see the agent's thought process\n\n### The Critical Trade-Off\n\n**Common misconception:** \"Agents are always better than static pipelines.\"\n\n**Reality check:** \n- Agents add **3-10s latency** (vs 300ms static)\n- Cost **5-10x more** ($0.01-0.03 vs $0.002 per query)\n- Harder to debug (probabilistic reasoning)\n\n**Use agents only when queries genuinely require multi-step reasoning or tool use.**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Section 2: Demonstrate Tool Registry\n\n# Get the tool registry\nfrom l2_m10_react_pattern_implementation import get_tools, calculator_tool, industry_data_tool\n\ntools = get_tools()\n\nprint(\"=== Tool Registry ===\")\nprint(f\"Number of tools: {len(tools)}\\n\")\n\nfor tool in tools:\n    print(f\"Tool: {tool.name}\")\n    print(f\"Description: {tool.description[:80]}...\")\n    print()\n\n# Test each tool independently\nprint(\"\\n=== Tool Tests ===\\n\")\n\n# Test 1: Calculator\nprint(\"[Test 1] Calculator Tool\")\nresult = calculator_tool(\"125000 * 1.15\")\nprint(f\"Input: 125000 * 1.15\")\nprint(f\"Output: {result}\")\nprint(f\"Output type: {type(result).__name__}\")  # Must be 'str'\nprint()\n\n# Test 2: Industry Data\nprint(\"[Test 2] Industry Data Tool\")\nresult = industry_data_tool(\"SaaS,growth_rate\")\nprint(f\"Input: SaaS,growth_rate\")\nprint(f\"Output: {result}\")\nprint()\n\n# Expected:\n# - 3 tools registered (RAG_Search, Calculator, Industry_Data)\n# - Calculator returns: \"Calculation: 125000 * 1.15 = 143,750.00\"\n# - Industry_Data returns: \"Industry benchmark for SaaS - growth_rate: 25-35% YoY\"\n# - All outputs are strings (not dicts/JSON)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Section 2: Tool Registry - Building the Agent's Capabilities\n\n### What Are Tools?\n\nTools are **functions** that the agent can call to accomplish tasks. Each tool:\n- Has a **clear name** (e.g., `RAG_Search`, `Calculator`)\n- Has a **description** explaining when to use it\n- Takes **input** and returns **plain text output** (not JSON!)\n- Must be **reliable** (>95% success rate in production)\n\n### The Three Core Tools\n\n1. **RAG_Search** - Wraps your Level 1 semantic search pipeline\n2. **Calculator** - Safely evaluates mathematical expressions\n3. **Industry_Data** - Fetches external benchmark data\n\n### Critical Design Decision\n\nTools **must return plain text**, not structured data (dict/JSON). Why?\n- The LLM needs to **read** the result as natural language\n- Structured data causes **parsing failures** (Failure #4 in script)\n- Plain text with interpretation is more reliable\n\n### Tool Output Examples\n\n‚ùå **BAD** (structured):\n```python\n{\"result\": 143750.0, \"formatted\": \"$143,750.00\"}\n```\n\n‚úÖ **GOOD** (plain text):\n```python\n\"Calculation: 125000 * 1.15 = 143,750.00\"\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10.1: ReAct Pattern Implementation\n",
    "## Agentic RAG with Thought ‚Üí Action ‚Üí Observation Reasoning Loop\n",
    "\n",
    "**Based on:** augmented_M10_VideoM10_1_ReAct_Pat.md  \n",
    "**Duration:** 42 minutes  \n",
    "**Level:** 3 (requires Level 1 M1.4 and Level 2 completion)\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Execute **Thought ‚Üí Action ‚Üí Observation cycles** for complex queries\n",
    "- Build a **tool registry** with RAG search, calculation, and API capabilities\n",
    "- Create **agent executors** that autonomously select and run appropriate tools\n",
    "- Debug **5 common agent failures**: infinite loops, wrong tool selection, state corruption, parsing failures, missed stop conditions\n",
    "- Recognize **when NOT to use** agentic patterns versus static pipelines\n",
    "\n",
    "**Important:** We'll be brutally honest about when agentic RAG is overkill‚Äîbecause 90% of queries don't need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Introduction & Problem Statement\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "Your Level 1 static RAG pipeline works beautifully for straightforward questions like \"What is our refund policy?\" But it **fails on complex queries** that require:\n",
    "\n",
    "1. **Multiple information sources** (internal docs + external benchmarks)\n",
    "2. **Calculations** (percentage differences, comparisons)\n",
    "3. **Multi-step reasoning** (gather ‚Üí calculate ‚Üí synthesize)\n",
    "\n",
    "Example query that breaks static pipelines:\n",
    "> *\"Compare our Q3 revenue to industry benchmarks, calculate the percentage difference, and suggest three growth strategies based on our current market position.\"*\n",
    "\n",
    "### The ReAct Solution\n",
    "\n",
    "The **ReAct pattern** (Reasoning and Acting) gives your RAG system the ability to:\n",
    "- **Think** about what tools it needs\n",
    "- **Act** by executing those tools in sequence\n",
    "- **Observe** results and decide on next steps\n",
    "- **Repeat** until the question is answered\n",
    "\n",
    "### Real-World Analogy\n",
    "\n",
    "Think of a detective solving a case:\n",
    "1. **Thought:** \"I need to check the suspect's alibi\"\n",
    "2. **Action:** Interview witnesses\n",
    "3. **Observation:** \"The alibi checks out, but there's a timeline gap\"\n",
    "4. **Thought:** \"I should examine phone records for that time period\"\n",
    "5. **Action:** Request phone records\n",
    "6. **Observation:** \"Multiple calls to an unknown number\"\n",
    "...and so on until conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import required modules\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Import our implementation\n",
    "from config import Config\n",
    "from l2_m10_react_pattern_implementation import (\n",
    "    get_tools,\n",
    "    StatefulReActAgent\n",
    ")\n",
    "\n",
    "print(\"‚úì Imports successful\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "for key, val in Config.get_info().items():\n",
    "    print(f\"  {key}: {val}\")\n",
    "\n",
    "# Expected: Module imports work, configuration displays"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}