{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9.3: Hypothetical Document Embeddings (HyDE)\n",
    "\n",
    "**Duration:** ~45 minutes  \n",
    "**Level:** 3 (MasteryX)  \n",
    "**Prerequisites:** Level 1 M1.1, M9.1, M9.2\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you'll be able to:\n",
    "- Implement HyDE: generate hypothetical answers with LLMs, embed them, and search\n",
    "- Build hybrid retrieval combining HyDE with traditional dense search\n",
    "- Create a performance comparison framework to measure HyDE effectiveness\n",
    "- Implement dynamic routing that decides when to use HyDE vs traditional retrieval\n",
    "- **Critical:** Understand when HyDE helps vs hurts (only 20% of queries benefit)\n",
    "- **Important:** Recognize the cost and latency trade-offs (adds 500-1000ms, costs $0.001-0.005 per query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Arc\n\n",
    "**Purpose:** Master Hypothetical Document Embeddings (HyDE) - an advanced retrieval technique that bridges vocabulary mismatch by generating hypothetical answers before embedding.\n\n",
    "**Concepts Covered:**\n",
    "- Hypothesis generation with LLMs for vocabulary translation\n",
    "- Hybrid retrieval combining HyDE with traditional dense search\n",
    "- Query classification for adaptive routing (only 20-30% queries benefit)\n",
    "- Performance comparison and cost-benefit analysis\n",
    "- 5 common production failures and fixes\n\n",
    "**After Completing:**\n",
    "You'll understand when HyDE helps (conceptual queries, vocabulary mismatch) vs. when it hurts (factoid queries, well-phrased inputs), and implement intelligent routing that achieves 15-40% precision gains while avoiding the 500-1000ms latency penalty for queries that don't benefit.\n\n",
    "**Context in Track L3.M9:**\n",
    "This module builds on M9.1 (Query Decomposition) and M9.2 (Multi-Hop Retrieval), adding vocabulary bridging capabilities. You're now handling complex, multi-part questions (M9.2) with intelligent hypothesis generation (M9.3) before moving to advanced reranking (M9.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction & Problem Statement\n",
    "\n",
    "### The Vocabulary Mismatch Problem\n",
    "\n",
    "You've built query decomposition (M9.1) and multi-hop retrieval (M9.2). Your advanced RAG system can handle complex questions. But here's a problem you're still hitting: **vocabulary mismatch**.\n",
    "\n",
    "**Example:**\n",
    "- **User asks:** \"What are the tax implications of stock options?\"\n",
    "- **Documents use:** \"equity compensation taxation framework under IRC Section 422\"\n",
    "\n",
    "Traditional dense retrieval embeds the user's question directly and searches for similar document embeddings. But user questions and document answers live in different semantic spaces.\n",
    "\n",
    "### The HyDE Solution\n",
    "\n",
    "Instead of embedding the question, we:\n",
    "1. Generate a hypothetical answer first (using LLM)\n",
    "2. Embed the hypothetical answer\n",
    "3. Search for documents similar to that answer\n",
    "\n",
    "You're searching in answer-space, not question-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# OFFLINE mode for L3 consistency\nimport os\nOFFLINE = os.getenv(\"OFFLINE\", \"false\").lower() == \"true\"\nif OFFLINE:\n    print(\"\u26a0\ufe0f  Running in OFFLINE mode \u2014 OpenAI/LLM calls will be skipped (mocked).\")\n    print(\"   Set OFFLINE=false to enable API calls.\\n\")\n\n# Setup: Import dependencies and check environment\nimport os\nimport json\nimport sys\n\n# Check Python version\nprint(f\"Python: {sys.version}\")\n\n# Check for required API keys\nhas_openai = bool(os.getenv(\"OPENAI_API_KEY\"))\nhas_pinecone = bool(os.getenv(\"PINECONE_API_KEY\"))\n\nprint(f\"\\n\u2713 OpenAI API Key: {'Found' if has_openai else '\u26a0\ufe0f  Not set'}\")\nprint(f\"\u2713 Pinecone API Key: {'Found' if has_pinecone else '\u26a0\ufe0f  Not set (optional)'}\")\n\nif not has_openai:\n    print(\"\\n\u26a0\ufe0f  Set OPENAI_API_KEY to run examples\")\n\n# Expected: Python 3.8+, OpenAI key found"
  },
  {
   "cell_type": "markdown",
   "source": "## Section 2: Prerequisites & Setup\n\n### Dependencies Check\n\nBefore we dive in, let's verify you have the foundation:\n- \u2705 Understanding of vector embeddings and semantic similarity (Level 1 M1.1)\n- \u2705 Query transformation techniques (M9.1)\n- \u2705 Multi-stage retrieval patterns (M9.2)\n\n### What We're Adding Today\n\nYour Level 3 system currently has advanced query decomposition and multi-hop retrieval. **The gap:** vocabulary mismatch between user queries and formal documents.\n\n**Today's solution:** HyDE capability that generates hypothetical formal answers first, improving retrieval quality by 15-40% for vocabulary-mismatched queries.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Verify dependencies\ntry:\n    import openai\n    print(f\"\u2713 openai: {openai.__version__}\")\nexcept ImportError:\n    print(\"\u2717 openai not installed. Run: pip install openai\")\n\ntry:\n    from pinecone import Pinecone\n    print(f\"\u2713 pinecone-client installed\")\nexcept ImportError:\n    print(\"\u26a0\ufe0f  pinecone-client not installed (optional)\")\n\n# Import our module\nfrom src.l3_m9_hypothetical_document_embeddings import (\n    HyDEGenerator,\n    HyDERetriever,\n    HybridHyDERetriever,\n    QueryClassifier,\n    AdaptiveHyDERetriever\n)\nprint(\"\u2713 Module imported successfully\")\n\n# Expected: All dependencies installed, module imports without errors",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 3: Theory Foundation\n\n### Core Concept: Bridging Semantic Spaces\n\n**The core insight:** User queries and document answers live in different parts of the embedding space.\n- **Queries** are questions: \"What is X?\"\n- **Documents** are statements: \"X is defined as...\"\n\nTraditional dense retrieval embeds your question and searches for similar questions in the docs \u2014 but docs don't contain questions, they contain answers!\n\n### Real-World Analogy\n\nImagine you're in a library searching for books about climate change.\n- **Traditional search:** Holding up a sign saying \"I want to learn about climate change\" and looking for books with similar signs\n- **HyDE:** Writing a hypothetical one-page summary of what a good climate change book would say, then finding books that match that summary\n\nYou're searching in answer-space, not question-space.\n\n### How HyDE Works\n\n1. **User Query:** \"What are tax implications of stock options?\"\n2. **Generate Hypothesis (LLM):** \"Stock option taxation follows IRS code section 422 for ISOs and 83 for NSOs. Upon exercise, income recognition depends on holding period...\"\n3. **Embed Hypothesis:** Convert to vector\n4. **Search Vector DB:** Find documents similar to hypothesis\n5. **Return Results:** Documents are more relevant (answer-to-answer matching)\n\n### Why This Matters for Production\n\n- **Vocabulary bridging:** Translates informal user queries to formal document language\n- **Domain adaptation:** Works without retraining embeddings on your specific domain\n- **Precision improvement:** 15-40% better retrieval quality for vocabulary-mismatched queries\n\n**Common misconception:** \"HyDE always improves retrieval.\" **Wrong.** HyDE helps with vocabulary mismatch but can hurt precision on queries that are already well-phrased or when the hypothesis is poor quality.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Section 4: Hands-On Implementation\n\nWe'll build HyDE step by step, integrating with your existing M9.2 retrieval system.\n\n### Step 1: Hypothesis Generation\n\nFirst, let's build an LLM-powered hypothesis generator that transforms user queries into document-style answers.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Step 1: Test hypothesis generation\nif has_openai:\n    generator = HyDEGenerator(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n    \n    query = \"What are the tax implications of stock options?\"\n    result = generator.generate_hypothesis(query)\n    \n    print(f\"Query: {query}\")\n    print(f\"\\nHypothesis (first 200 chars):\\n{result['hypothesis'][:200]}...\")\n    print(f\"\\n\u2713 Time: {result['generation_time_ms']:.0f}ms\")\n    print(f\"\u2713 Tokens: {result['tokens_used']}\")\n    print(f\"\u2713 Model: {result['model']}\")\nelse:\n    print(\"\u26a0\ufe0f  Skipping (no OPENAI_API_KEY)\")\n\n# Expected: Formal document-style hypothesis in ~500-800ms",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: HyDE-Based Retrieval\n\nNow integrate with Pinecone vector database for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Test HyDE retrieval (skips if no Pinecone)\n",
    "if has_openai:\n",
    "    retriever = HyDERetriever(\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        pinecone_api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
    "        pinecone_index_name=os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "    )\n",
    "    \n",
    "    query = \"What are the tax implications of stock options?\"\n",
    "    result = retriever.retrieve_with_hyde(query, top_k=5)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"\\nHypothesis: {result['hypothesis'][:150]}...\")\n",
    "    print(f\"\\nPerformance:\")\n",
    "    print(f\"  Total: {result['performance']['total_time_ms']:.0f}ms\")\n",
    "    print(f\"  Hypothesis: {result['performance']['hypothesis_generation_ms']:.0f}ms\")\n",
    "    print(f\"  Embedding: {result['performance']['embedding_time_ms']:.0f}ms\")\n",
    "    print(f\"  Search: {result['performance']['search_time_ms']:.0f}ms\")\n",
    "    print(f\"\\nResults: {result['metadata']['num_results']}\")\n",
    "    if result['metadata']['skipped_search']:\n",
    "        print(\"\u26a0\ufe0f  Vector search skipped (no Pinecone)\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Skipping (no OPENAI_API_KEY)\")\n",
    "\n",
    "# Expected: Hypothesis generated + search results (or graceful skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Hybrid Retrieval\n\nCombine HyDE with traditional retrieval for best of both worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Test hybrid retrieval\n",
    "if has_openai:\n",
    "    hybrid = HybridHyDERetriever(\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        pinecone_api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
    "        pinecone_index_name=os.getenv(\"PINECONE_INDEX_NAME\"),\n",
    "        hyde_weight=0.6,\n",
    "        traditional_weight=0.4\n",
    "    )\n",
    "    \n",
    "    query = \"How does ISO taxation work?\"\n",
    "    result = hybrid.retrieve_hybrid(query, top_k=5)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"\\nPerformance:\")\n",
    "    print(f\"  Total: {result['performance']['total_time_ms']:.0f}ms\")\n",
    "    print(f\"  HyDE: {result['performance']['hyde_time_ms']:.0f}ms\")\n",
    "    print(f\"  Traditional: {result['performance']['traditional_time_ms']:.0f}ms\")\n",
    "    print(f\"\\nSource breakdown:\")\n",
    "    print(f\"  HyDE: {result['metadata']['hyde_count']}\")\n",
    "    print(f\"  Traditional: {result['metadata']['traditional_count']}\")\n",
    "    print(f\"  Both: {result['metadata']['both_count']}\")\n",
    "    print(f\"\\n\u2713 Merged {len(result['results'])} results\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Skipping (no OPENAI_API_KEY)\")\n",
    "\n",
    "# Expected: Combined results from both methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Query Classification\n\nBuild a classifier to determine when HyDE should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Test query classification\n",
    "classifier = QueryClassifier()\n",
    "\n",
    "test_queries = [\n",
    "    (\"What are the implications of equity compensation?\", \"conceptual\"),\n",
    "    (\"When was the 2023 tax deadline?\", \"factoid\"),\n",
    "    (\"How does ISO taxation differ from NSO?\", \"conceptual\"),\n",
    "    (\"List all required tax forms\", \"factoid\")\n",
    "]\n",
    "\n",
    "print(\"Query Classification Results:\\n\" + \"=\"*60)\n",
    "for query, expected_type in test_queries:\n",
    "    result = classifier.should_use_hyde(query)\n",
    "    method = \"HyDE\" if result['use_hyde'] else \"Traditional\"\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"  Expected: {expected_type}\")\n",
    "    print(f\"  Decision: {method}\")\n",
    "    print(f\"  Confidence: {result['confidence']:.2f}\")\n",
    "    print(f\"  Signals: +{result['beneficial_signals']} beneficial, +{result['harmful_signals']} harmful\")\n",
    "\n",
    "# Expected: Conceptual queries \u2192 HyDE, Factoid queries \u2192 Traditional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Adaptive Routing\n\nPut it all together with automatic method selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Test adaptive retrieval\n",
    "if has_openai:\n",
    "    adaptive = AdaptiveHyDERetriever(\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        pinecone_api_key=os.getenv(\"PINECONE_API_KEY\"),\n",
    "        pinecone_index_name=os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "    )\n",
    "    \n",
    "    test_queries = [\n",
    "        \"What are the tax implications of stock options?\",  # Conceptual\n",
    "        \"When was the 2023 tax deadline?\"  # Factoid\n",
    "    ]\n",
    "    \n",
    "    print(\"Adaptive Routing Test:\\n\" + \"=\"*60)\n",
    "    for query in test_queries:\n",
    "        result = adaptive.retrieve(query, top_k=3)\n",
    "        \n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(f\"  Method: {result['routing']['method_used']}\")\n",
    "        print(f\"  Reasoning: {result['routing']['reasoning']}\")\n",
    "        print(f\"  Latency: {result['performance']['total_time_ms']:.0f}ms\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Skipping (no OPENAI_API_KEY)\")\n",
    "\n",
    "# Expected: Auto-routes to best method for each query type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Reality Check - What HyDE DOESN'T Do\n\n",
    "Let's be honest about limitations. HyDE is powerful for specific scenarios, but it's NOT a silver bullet.\n\n",
    "### What HyDE DOESN'T Do:\n\n",
    "**1. HyDE doesn't help when queries are already well-phrased**\n",
    "- Example: Legal professionals searching with precise legal terms\n",
    "- Impact: Adds 500-1000ms latency with ZERO quality improvement\n",
    "- Workaround: Use query classification to skip HyDE\n\n",
    "**2. HyDE reduces precision when hypotheses are generic or wrong**\n",
    "- Technical reason: LLM generates vague or hallucinated hypotheses\n",
    "- Real consequence: 15-25% precision DROP on factoid queries\n",
    "- When you'll hit this: Queries about specific dates, numbers, names\n",
    "- What to do: Fall back to traditional retrieval\n\n",
    "**3. HyDE adds 500-1000ms latency overhead**\n",
    "- Why: LLM inference takes 400-800ms, plus embedding time\n",
    "- Impact: Noticeable delay - users expect <500ms, you're delivering 800-1200ms\n",
    "- When you'll hit this: Every single HyDE query\n",
    "- Workaround: Only use HyDE for queries where quality justifies latency\n\n",
    "### Trade-offs You Accepted:\n\n",
    "- **Complexity:** 300+ lines of code, 4 new components\n",
    "- **Cost:** $0.001-0.005 per query (vs $0.0001 for embedding only)\n",
    "- **Latency:** 500-1000ms added to every HyDE query\n",
    "- **Precision risk:** 20-30% of queries see WORSE results\n\n",
    "### When This Approach Breaks:\n\n",
    "HyDE becomes insufficient when:\n",
    "- **Latency requirements <500ms** (HyDE can't meet this)\n",
    "- **Query diversity >80% factoid** (HyDE helps <20%)\n",
    "- **Budget <$0.0005/query** (HyDE is 10-50x more expensive)\n",
    "- **Niche domains** (GPT-4 generates poor hypotheses)\n\n",
    "**Bottom line:** HyDE is right for knowledge bases with conceptual queries from non-experts. For experts or specialized content, skip HyDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Alternative Solutions\n\n",
    "HyDE isn't the only way to handle vocabulary mismatch.\n\n",
    "### Alternative 1: Query Expansion (Free, Fast)\n",
    "\n",
    "**Best for:** <200ms latency, budget <$0.001/query\n",
    "\n",
    "Uses thesaurus/WordNet to expand query terms with synonyms. No LLM needed.\n",
    "\n",
    "**Trade-offs:**\n",
    "- \u2705 Very fast (5-10ms), cheap (free), simple\n",
    "- \u2705 Works offline, no API calls\n",
    "- \u274c Only handles literal synonyms, misses semantic similarity\n",
    "- \u274c Can add noisy terms that hurt precision\n\n",
    "**Cost:** Free, 5-10ms latency\n\n",
    "### Alternative 2: Fine-Tuned Embeddings\n\n",
    "**Best for:** Domain-specific, have labeled data, need consistent performance\n",
    "\n",
    "Fine-tune embedding models on your domain using contrastive learning.\n",
    "\n",
    "**Trade-offs:**\n",
    "- \u2705 Solves mismatch at embedding level (no latency overhead)\n",
    "- \u2705 Consistent performance (no LLM variability)\n",
    "- \u2705 One-time cost\n",
    "- \u274c Requires labeled training data (500-5000 pairs)\n",
    "- \u274c Training cost: $50-500\n",
    "- \u274c Need ML expertise\n\n",
    "**Cost:** $50-500 one-time, $0.0001/query inference, no latency overhead\n\n",
    "### Alternative 3: Hybrid BM25+Dense\n",
    "\n",
    "**Best for:** Want proven approach, can't afford HyDE latency\n",
    "\n",
    "Combine BM25 (keyword) with dense embeddings. BM25 catches exact matches, dense catches semantic.\n",
    "\n",
    "**Trade-offs:**\n",
    "- \u2705 Proven (used by Elasticsearch, Algolia)\n",
    "- \u2705 Fast (150-200ms, no LLM call)\n",
    "- \u2705 Handles both keyword and semantic mismatch\n",
    "- \u274c Requires BM25 index (2x storage)\n",
    "- \u274c Still doesn't handle vocabulary mismatch as well as HyDE\n\n",
    "**Cost:** 2x storage, $0.0002/query, 150-200ms\n\n",
    "### Decision Framework:\n\n",
    "```\n",
    "Latency <200ms? \u2192 Query Expansion\n",
    "Have labeled data? \u2192 Fine-Tuned Embeddings\n",
    "Need keyword+semantic? \u2192 Hybrid BM25+Dense\n",
    "Conceptual from non-experts? \u2192 HyDE\n",
    "Factoid or experts? \u2192 Traditional\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Common Failures & Fixes\n\n",
    "Let's debug the 5 most common production failures.\n\n",
    "### Failure 1: Hypothetical Answers Too Generic\n\n",
    "**Symptom:** Vague hypotheses like \"It works by following a process...\"\n",
    "\n",
    "**Root cause:** Query too vague or LLM lacks domain context\n",
    "\n",
    "**Fix:**\n",
    "```python\n",
    "# Provide domain context and examples\n",
    "generator.generate_hypothesis(\n",
    "    query=query,\n",
    "    domain_context=\"Financial docs use IRC sections\",\n",
    "    example_documents=[doc1, doc2]\n",
    ")\n",
    "```\n\n",
    "**Prevention:** Always provide domain context, reject hypotheses <50 words\n\n",
    "### Failure 2: Precision Drop on Factoid Queries\n\n",
    "**Symptom:** \"When was X?\" queries return wrong results\n",
    "\n",
    "**Root cause:** LLM hallucines wrong fact or generates too broad answer\n",
    "\n",
    "**Fix:** Use query classification (Step 5 implementation)\n",
    "\n",
    "**Prevention:** ALWAYS use adaptive routing\n\n",
    "### Failure 3: Latency Timeouts\n\n",
    "**Symptom:** 504 Gateway Timeout, p95 latency >2s\n",
    "\n",
    "**Root cause:** OpenAI API latency spikes under load\n",
    "\n",
    "**Fix:**\n",
    "- Implement hypothesis caching (Redis)\n",
    "- Use async generation with proper timeouts\n",
    "- Set application timeout to 3-5s (not 1.5s)\n\n",
    "**Prevention:** Cache hypotheses (1hr TTL), monitor p95 latency\n\n",
    "### Failure 4: Poor Domain Quality\n\n",
    "**Symptom:** Wrong hypotheses for specialized domains\n",
    "\n",
    "**Root cause:** GPT-4 lacks your domain expertise\n",
    "\n",
    "**Fix:** Use RAG-augmented hypothesis generation (retrieve context first, then generate)\n\n",
    "**Prevention:** For specialized domains, always use contextual generation\n\n",
    "### Failure 5: Wrong Routing Decisions\n\n",
    "**Symptom:** Classification accuracy <80%\n",
    "\n",
    "**Root cause:** Regex patterns too brittle\n",
    "\n",
    "**Fix:** Upgrade to LLM-based classification (50-100ms overhead, 90% accuracy)\n",
    "\n",
    "**Prevention:** Monitor classification accuracy, iterate on patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Common failure - Generic hypothesis\n",
    "if has_openai:\n",
    "    generator = HyDEGenerator(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    # Vague query \u2192 generic hypothesis\n",
    "    vague_query = \"How does it work?\"\n",
    "    result = generator.generate_hypothesis(vague_query)\n",
    "    print(f\"Vague Query: {vague_query}\")\n",
    "    print(f\"Hypothesis: {result['hypothesis'][:150]}...\")\n",
    "    print(\"\u26a0\ufe0f  Generic hypothesis (as expected)\\n\")\n",
    "    \n",
    "    # Specific query with context \u2192 better hypothesis\n",
    "    specific_query = \"How does ISO taxation work?\"\n",
    "    result = generator.generate_hypothesis(\n",
    "        specific_query,\n",
    "        domain_context=\"Tax documents use IRC sections and formal legal language\"\n",
    "    )\n",
    "    print(f\"Specific Query: {specific_query}\")\n",
    "    print(f\"Hypothesis: {result['hypothesis'][:150]}...\")\n",
    "    print(\"\u2713 Better hypothesis with context\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Skipping (no OPENAI_API_KEY)\")\n",
    "\n",
    "# Expected: Generic hypothesis for vague query, better for specific+context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Decision Card - Quick Reference\n\n",
    "### \u2705 BENEFIT\n",
    "Bridges vocabulary mismatch; 15-40% precision gain for conceptual queries; works without retraining embeddings; effective for compliance/legal/technical domains where formal and informal language differ.\n\n",
    "### \u274c LIMITATION\n",
    "Adds 500-1000ms latency (cannot be eliminated); costs $0.001-0.005/query (10-50x traditional); reduces precision on factoid queries; only benefits 20-30% of queries; fails on highly specialized niche domains.\n\n",
    "### \ud83d\udcb0 COST\n",
    "**Implementation:** 6-8 hours development + 4-6 hours monitoring\n\n",
    "**Operational:** $100-2000/month OpenAI (1K-100K queries/day); $70-500/month Pinecone; $20-50/month Redis caching\n\n",
    "**Complexity:** 300+ lines of code, 4 new components (generator, classifier, merger, evaluator)\n\n",
    "### \ud83e\udd14 USE WHEN\n",
    "- Building knowledge base for non-expert users\n",
    "- Queries primarily conceptual (\"What/How/Why\")\n",
    "- Severe vocabulary mismatch (informal \u2194 formal)\n",
    "- Latency budget >700ms p95\n",
    "- Budget allows $0.001-0.005/query\n",
    "- Domain general enough for GPT-4\n\n",
    "### \ud83d\udeab AVOID WHEN\n",
    "- Latency requirement <500ms \u2192 use fine-tuned embeddings\n",
    "- Queries primarily factoid \u2192 traditional sufficient\n",
    "- Budget <$0.001/query \u2192 use query expansion or hybrid BM25\n",
    "- Highly specialized domain \u2192 fine-tuned embeddings better\n",
    "- Can afford fine-tuning \u2192 $500 one-time vs $1500/month ongoing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Production Considerations\n\n",
    "What changes when you scale to production?\n\n",
    "### Scaling Concerns\n\n",
    "**At 1,000 queries/day (small):**\n",
    "- Avg latency: 700ms acceptable\n",
    "- Cost: ~$100/month ($30 OpenAI + $70 Pinecone)\n",
    "- Monitoring: Track hypothesis success rate (>95%)\n\n",
    "**At 10,000 queries/day (medium):**\n",
    "- p95 latency: 1500ms (need optimization)\n",
    "- Cost: ~$280/month with 40% cache hit\n",
    "- Required: Redis caching, request coalescing, connection pooling\n\n",
    "**At 100,000+ queries/day (large):**\n",
    "- p95 latency: 2000ms (consider alternatives)\n",
    "- Cost: ~$2000/month\n",
    "- Recommendation: Seriously evaluate fine-tuned embeddings instead\n\n",
    "### Cost Optimization Tips:\n\n",
    "1. **Caching:** Save 40-50% on OpenAI costs (easy win)\n",
    "2. **Adaptive routing:** Only use HyDE for 20-30% \u2192 70% cost reduction\n",
    "3. **Cheaper model:** gpt-3.5-turbo 10x cheaper\n",
    "4. **At scale:** Fine-tuned embeddings ($500 one-time vs $1500/month)\n\n",
    "### Monitoring Requirements:\n\n",
    "**Must track:**\n",
    "- Hypothesis generation success rate (target >95%)\n",
    "- HyDE vs Traditional precision by query type\n",
    "- p95 latency (target <1500ms)\n",
    "- OpenAI API costs (target <$0.002/query)\n",
    "- Cache hit rate (target >40%)\n\n",
    "**Alert on:**\n",
    "- Hypothesis failure rate >5% for 10 min\n",
    "- p95 latency >2000ms for 5 min\n",
    "- HyDE precision worse than traditional\n",
    "- Daily OpenAI spend exceeds budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production readiness checklist\n",
    "print(\"Production Readiness Checklist\\n\" + \"=\"*60)\n",
    "\n",
    "checklist = {\n",
    "    \"Hypothesis caching (Redis, 1hr TTL)\": False,\n",
    "    \"Adaptive routing enabled\": True,  # We built this\n",
    "    \"Graceful fallback on timeout\": True,  # Module handles this\n",
    "    \"Request coalescing\": False,\n",
    "    \"Monitoring dashboard\": False,\n",
    "    \"Alerts configured\": False,\n",
    "    \"Runbook for failures\": False,\n",
    "    \"Budget approval\": False,\n",
    "    \"A/B test plan\": False\n",
    "}\n",
    "\n",
    "for item, completed in checklist.items():\n",
    "    status = \"\u2713\" if completed else \"\u26a0\ufe0f\"\n",
    "    print(f\"{status} {item}\")\n",
    "\n",
    "completed_count = sum(checklist.values())\n",
    "print(f\"\\nReady: {completed_count}/{len(checklist)} items\")\n",
    "print(\"\\n\u26a0\ufe0f  Complete remaining items before production deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: PractaThon Challenges\n\n",
    "Practice what you've learned. Choose your challenge level:\n\n",
    "### \ud83d\udfe2 EASY (90 minutes)\n",
    "**Goal:** Implement basic HyDE retrieval with caching\n\n",
    "**Requirements:**\n",
    "1. Implement HyDEGenerator and HyDERetriever\n",
    "2. Add Redis caching for hypotheses (1-hour TTL)\n",
    "3. Write tests comparing HyDE vs traditional on 5 sample queries\n",
    "4. Measure and report latency and precision\n\n",
    "**Success criteria:**\n",
    "- HyDE generates hypotheses in <800ms p95\n",
    "- Cache hit rate >30% after 20 queries\n",
    "- HyDE improves precision by >10% on 3/5 conceptual queries\n\n",
    "### \ud83d\udfe1 MEDIUM (2-3 hours)\n",
    "**Goal:** Build hybrid HyDE+Traditional with adaptive routing\n",
    "\n",
    "**Requirements:**\n",
    "1. Implement full hybrid retrieval system\n",
    "2. Build query classifier for adaptive routing\n",
    "3. Create performance comparison framework (precision, recall, MRR)\n",
    "4. Test on 20 diverse queries (10 conceptual, 10 factoid)\n",
    "5. Implement graceful fallback on timeout\n",
    "6. Write monitoring code for hypothesis quality\n\n",
    "**Success criteria:**\n",
    "- Adaptive routing correctly classifies >80% of queries\n",
    "- Hybrid improves precision by >15% on conceptual\n",
    "- No precision reduction on factoid queries\n",
    "- p95 latency <1500ms with proper timeout handling\n",
    "- Bonus: Caching reduces costs by >40%\n\n",
    "### \ud83d\udd34 HARD (5-6 hours)\n",
    "**Goal:** Production-ready HyDE with RAG-augmented hypothesis\n",
    "\n",
    "**Requirements:**\n",
    "1. Implement RAG-augmented hypothesis generation\n",
    "2. Build LLM-based query classifier (not regex)\n",
    "3. Implement async generation with request coalescing\n",
    "4. Create comprehensive evaluation suite\n",
    "5. Build monitoring dashboard\n",
    "6. Write runbook for all 5 common failures\n",
    "7. Load test: 100 concurrent queries with p95 <2s\n\n",
    "**Success criteria:**\n",
    "- HyDE improves precision by >20% on vocabulary-mismatch queries\n",
    "- Routing accuracy >90%\n",
    "- p95 latency <1500ms under 100 concurrent queries\n",
    "- Hypothesis generation failure rate <5%\n",
    "- Cost per query <$0.003 after optimizations\n",
    "- All 5 common failures handled gracefully\n",
    "- Bonus: A/B testing framework\n\n",
    "### Submission\n",
    "Push to GitHub with:\n",
    "- Working code (`python main.py`)\n",
    "- README with architecture decisions\n",
    "- Test results CSV\n",
    "- (Medium/Hard) Monitoring dashboard screenshots\n",
    "- (Hard) Runbook for common failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Wrap-Up & Next Steps\n\n",
    "### What You Built Today:\n\n",
    "\u2705 **HyDE pipeline:** LLM hypothesis generation \u2192 embedding \u2192 search (15-40% precision gain on conceptual queries)\n\n",
    "\u2705 **Hybrid retrieval:** Combining HyDE with traditional dense search (hedges risk)\n\n",
    "\u2705 **Performance comparison:** Framework measuring precision, recall, MRR by query type\n\n",
    "\u2705 **Adaptive routing:** Automatically chooses HyDE vs traditional based on query classification\n\n",
    "### What You Learned:\n\n",
    "\u2705 **When HyDE helps:** Vocabulary mismatch on conceptual queries from non-experts\n\n",
    "\u2705 **When HyDE hurts:** Factoid queries, well-phrased queries, latency-sensitive apps, tight budgets\n\n",
    "\u2705 **5 production failures:** Generic hypotheses, precision drops, timeouts, poor quality, routing errors\n\n",
    "\u2705 **3 alternatives:** Query expansion (free, fast), fine-tuned embeddings (no overhead), hybrid BM25+dense (proven)\n\n",
    "\u2705 **Critical insight:** Only 20-30% of queries benefit from HyDE - use adaptive routing\n\n",
    "### Your System Now:\n\n",
    "**Started with:** Traditional dense retrieval struggling with vocabulary mismatch\n\n",
    "**Now has:** Intelligent multi-strategy retrieval that automatically chooses the best approach for each query type, improving overall precision by 15-40% on difficult queries while maintaining speed on simple queries\n\n",
    "### Reality Check Reminder:\n\n",
    "HyDE adds 500-1000ms latency and costs $0.001-0.005 per query. It's powerful but expensive and slow. Use it judiciously through adaptive routing. For high-volume systems (>100K queries/day), seriously consider fine-tuned embeddings to eliminate latency and cost overhead.\n\n",
    "### Next Steps:\n\n",
    "1. **Complete PractaThon challenge** (choose your level - recommend Medium)\n",
    "2. **Test on your data** (run evaluation framework on 50 real queries)\n",
    "3. **Measure cost vs quality** (calculate if precision improvement justifies cost)\n",
    "4. **Next module: M9.4 - Advanced Reranking Strategies**\n",
    "   - Ensemble cross-encoders\n",
    "   - MMR diversity\n",
    "   - Recency boosting\n",
    "   - User preference learning\n\n",
    "**Great work!** You now have one of the most sophisticated retrieval systems. See you in M9.4!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"Module 9.3: HyDE - Summary\\n\" + \"=\"*60)\n",
    "print(\"\\n\u2713 Completed all sections\")\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"1. HyDE bridges vocabulary mismatch (15-40% gain)\")\n",
    "print(\"2. Adds 500-1000ms latency, costs $0.001-0.005/query\")\n",
    "print(\"3. Only 20-30% of queries benefit (use adaptive routing)\")\n",
    "print(\"4. Fails on: factoid queries, well-phrased queries, niche domains\")\n",
    "print(\"5. Alternatives: Query expansion, fine-tuned embeddings, hybrid BM25\")\n",
    "print(\"\\n\u2705 You're ready for M9.4: Advanced Reranking!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}