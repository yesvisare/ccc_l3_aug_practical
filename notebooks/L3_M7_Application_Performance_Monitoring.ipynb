{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7.2: Application Performance Monitoring\n",
    "\n",
    "**Duration:** 38 minutes  \n",
    "**Prerequisites:** Level 1 M2.3 (Prometheus/Grafana) + Level 2 M7.1 (OpenTelemetry Tracing)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Integrate Datadog APM with existing OpenTelemetry tracing (without double instrumentation)\n",
    "- Profile production code to find bottlenecks down to the function level (with <5% overhead)\n",
    "- Detect memory leaks and CPU hotspots in live systems (without crashing production)\n",
    "- Optimize database queries using APM query analysis\n",
    "- Understand when APM is overkill and what cheaper alternatives exist"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## \ud83d\udcda Learning Arc\n\n**Purpose**: Master Application Performance Monitoring (APM) to identify code-level bottlenecks in production RAG systems beyond what distributed tracing reveals.\n\n**Concepts Covered**:\n- Datadog APM integration with OpenTelemetry bridge (M7.1 compatibility)\n- Production-safe profiling configuration (1% capture, 10% sampling, <5% CPU overhead)\n- Memory leak detection with tracemalloc and continuous monitoring\n- Cost optimization strategies and alternative solutions (Grafana Tempo, py-spy)\n- Decision framework: When APM is essential vs. when it's premature optimization\n\n**After Completing**: You'll be able to deploy APM in production without crushing performance, detect memory leaks before they cause OOM kills, and make informed cost/benefit decisions about APM tooling.\n\n**Context in Track L3.M7**: Builds on M7.1 (Distributed Tracing) by adding function-level profiling. APM shows *why* a span is slow, while tracing shows *which* spans are slow.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 2: Prerequisites & Setup\n\nThis section verifies your environment is ready for APM profiling and establishes OFFLINE mode for local development.\n\n### Starting Point Verification\n\nYour Level 2 M7.1 system currently has:\n- OpenTelemetry tracing showing request flows\n- Traces visible in Jaeger UI\n- Spans tagged with custom attributes\n- Trace sampling configured (10-20%)\n\n**The gap:** When Jaeger shows a slow span, you can't see what's happening inside at the code level. APM fills this gap with function-level profiling.\n\n### Dependencies Installation\n\nWe'll use these libraries (all optional - the module works without them):\n- `ddtrace`: Datadog APM library (agentless approach)\n- `py-spy`: Low-overhead profiling tool for spot checks\n- `memory-profiler`: Memory leak detection utilities"
  },
  {
   "cell_type": "markdown",
   "source": "# Environment setup and OFFLINE mode check\nimport os\nimport sys\n\n# OFFLINE mode for L3 consistency\nOFFLINE = os.getenv(\"OFFLINE\", \"false\").lower() == \"true\"\nAPM_ENABLED = os.getenv(\"APM_ENABLED\", \"false\").lower() == \"true\"\n\nif OFFLINE or not APM_ENABLED:\n    print(\"\u26a0\ufe0f  Running in OFFLINE/APM_DISABLED mode\")\n    print(\"   Telemetry will not be exported to Datadog\")\n    print(\"   The module will demonstrate APM concepts without external services\")\n    print()\n\nprint(\"Checking dependencies...\")\n\n# Check ddtrace (optional - APM will be disabled if missing)\ntry:\n    import ddtrace\n    print(f\"\u2713 ddtrace: {ddtrace.__version__}\")\nexcept ImportError:\n    print(\"\u26a0\ufe0f  ddtrace not available - APM features disabled\")\n\n# Check OpenTelemetry (M7.1 prerequisite)\ntry:\n    import opentelemetry\n    print(f\"\u2713 OpenTelemetry: available\")\nexcept ImportError:\n    print(\"\u26a0\ufe0f  OpenTelemetry not available - install for M7.1 compatibility\")\n\n# Expected: ddtrace 2.x.x or higher, OpenTelemetry available\n# Note: APM will work without these libraries in demonstration mode",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Verify dependencies installation\nimport sys\n\nprint(\"Checking dependencies...\")\n\n# Check ddtrace (optional - APM will be disabled if missing)\ntry:\n    import ddtrace\n    print(f\"\u2713 ddtrace: {ddtrace.__version__}\")\nexcept ImportError:\n    print(\"\u26a0\ufe0f  ddtrace not available - APM features disabled\")\n\n# Check OpenTelemetry (M7.1 prerequisite)\ntry:\n    import opentelemetry\n    print(f\"\u2713 OpenTelemetry: available\")\nexcept ImportError:\n    print(\"\u26a0\ufe0f  OpenTelemetry not available - install for M7.1 compatibility\")\n\n# Expected: ddtrace 2.x.x or higher, OpenTelemetry available",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 3: Theory Foundation\n\n### APM vs Tracing\n\n**Analogy: Debugging a Traffic Jam**\n\n- **Metrics** (Prometheus): \"Highway has 1,000 cars/hour, average speed 20mph\"\n- **Tracing** (OpenTelemetry): \"Car #47 took 45 minutes from entrance to exit, passing through zones A \u2192 B \u2192 C\"\n- **APM**: \"Car #47 spent 30 of those 45 minutes stopped in zone B because the left lane was blocked by a stalled truck at mile marker 23.7\"\n\n### How APM Works\n\n```\nYour Python App\n\u251c\u2500\u2500 OpenTelemetry (Traces)\n\u2502   \u2514\u2500\u2500 Span: \"process_query\" - 2.5s\n\u2502\n\u2514\u2500\u2500 Datadog APM (Profiling)\n    \u2514\u2500\u2500 WITHIN that span:\n        \u251c\u2500\u2500 Function: embedding_model() - 200ms\n        \u251c\u2500\u2500 Function: chunk_filter() - 2.1s \u26a0\ufe0f\n        \u2502   \u2514\u2500\u2500 Line 187: nested loop - 1.8s \u26a0\ufe0f\u26a0\ufe0f\n        \u2514\u2500\u2500 Function: format_response() - 200ms\n```\n\n**Process:**\n1. APM agent samples your Python process (default: 100 samples/second)\n2. Each sample captures the call stack (which functions are executing)\n3. Over time, you get a statistical profile: \"85% of time is in chunk_filter()\"\n4. APM correlates this with your OpenTelemetry traces\n\n### Key Distinction\n\n**APM COMPLEMENTS tracing, doesn't replace it:**\n- Tracing: Shows request flow between services (the 'what' and 'where')\n- APM: Shows code execution within a service (the 'why')",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Section 4: Hands-On Implementation\n\nIn this section, you'll initialize APM and run profiled RAG queries to see code-level bottleneck detection in action.\n\n### Step 1: Initialize APM Manager\n\nThe APM Manager handles:\n- Datadog tracer configuration with production-safe defaults\n- OpenTelemetry compatibility bridge (reuses M7.1 traces)\n- Continuous profiler startup/shutdown with graceful degradation\n- Production safety limits (max 5% CPU overhead, 1% sampling)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 4: Hands-On Implementation\n\n### Step 1: Initialize APM Manager\n\nThe APM Manager handles:\n- Datadog tracer configuration\n- OpenTelemetry compatibility bridge\n- Continuous profiler startup/shutdown\n- Production safety limits (max 5% CPU overhead)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Initialize APM Manager\nfrom src.l3_m7_application_performance_monitoring import apm_manager\n\nprint(\"Initializing APM...\")\nsuccess = apm_manager.initialize()\n\nif success:\n    print(\"\u2705 APM initialized successfully\")\n    print(f\"   Service: {apm_config.DD_SERVICE}\")\n    print(f\"   Environment: {apm_config.DD_ENV}\")\nelse:\n    print(\"\u26a0\ufe0f  APM initialization skipped\")\n    print(\"   Reason: No DD_API_KEY configured or ddtrace not installed\")\n    print(\"   Pipeline will work without APM profiling\")\n\n# Expected: APM initializes if keys configured, otherwise gracefully skips",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Step 2: Profiled RAG Pipeline\n\nThe ProfiledRAGPipeline demonstrates:\n- Custom profiling with `@tracer.wrap()` decorators\n- Span tagging (user_id, query_length)\n- Exception tracking\n- O(n\u00b2) bottleneck simulation for APM detection\n\n**What APM will show:**\n```\nSpan: rag.query - 2,547ms\n\u251c\u2500 Span: rag.embed_query - 201ms\n\u251c\u2500 Span: rag.search_vectordb - 304ms\n\u251c\u2500 Span: rag.process_context - 1,893ms \u26a0\ufe0f\n\u2502  \u2514\u2500 Profile: _remove_overlapping_chunks() - 1,750ms\n\u2502     \u2514\u2500 Line 503: nested loop hotspot\n\u2514\u2500 Span: rag.generate_response - 503ms\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Section 5: Memory Profiling & Leak Detection\n\nMemory leaks are production incidents waiting to happen. This section demonstrates continuous memory monitoring to catch leaks before they cause OOM kills.\n\n### Why Memory Leaks Matter\n\nMemory leaks are silent killers in production:\n- Gradual memory growth over hours/days\n- Process eventually OOM killed (Out Of Memory)\n- Difficult to debug without profiling tools\n\n### Detection Strategy\n\n1. **Baseline tracking**: Record memory at startup\n2. **Periodic sampling**: Check memory every N requests\n3. **Growth analysis**: Alert if growth >10MB/hour\n4. **Leak identification**: Use objgraph to find growing objects\n\n### Memory Profiling with tracemalloc\n\nPython's built-in tracemalloc module provides:\n- Memory allocation tracking\n- Low overhead (<5% CPU impact)\n- Peak memory and growth metrics\n- Line-level attribution",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 5: Memory Profiling & Leak Detection\n\n### Why Memory Leaks Matter\n\nMemory leaks are silent killers in production:\n- Gradual memory growth over hours/days\n- Process eventually OOM killed\n- Difficult to debug without profiling\n\n### Detection Strategy\n\n1. **Baseline tracking**: Record memory at startup\n2. **Periodic sampling**: Check memory every N requests\n3. **Growth analysis**: Alert if growth >10MB/hour\n4. **Leak identification**: Use objgraph to find growing objects\n\n### Memory Profiling with tracemalloc\n\n- Python built-in module\n- Tracks memory allocations\n- Low overhead (<5%)\n- Shows peak memory and growth",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Section 6: Reality Check\n\nThis section is critical: understanding what APM can't do and when it breaks is as important as knowing what it can do. Don't skip this.\n\n### What This DOESN'T Do\n\n1. **Replace code optimization**: APM shows you the problem, you still have to fix it\n2. **Eliminate load testing**: Profiling shows behavior under load, not capacity limits\n3. **Diagnose network issues**: Use distributed tracing (M7.1) for cross-service problems\n\n### Trade-offs You Accepted\n\nEvery tool has costs. APM's trade-offs:\n1. **Performance overhead**: 2-5% CPU even with conservative sampling (1% profiling, 10% traces)\n2. **Cost**: $51-100/month minimum, rising to $300+ at scale\n3. **Complexity**: 300+ lines of config code, requires profiling expertise\n4. **Data privacy**: Telemetry sent to Datadog (third-party service)\n\n### When This Approach Breaks\n\nReal-world failure scenarios with solutions:\n\n**Scenario 1: APM Overhead Crushes Performance**\n- Production-safe config: 1% profiling, 10% sampling\n- If you increase to 10% profiling + 100% sampling \u2192 5-15% slowdown\n- **Solution**: Always use production-safe defaults, load test first\n\n**Scenario 2: Cost Explosion**\n- Expected: $51/month for small deployment\n- At 100K requests/hour: $300-500/month due to per-span fees ($5 per 1M spans)\n- **Solution**: Adaptive sampling (reduce rate at high traffic)\n\n**Scenario 3: Memory Leak Not Detected**\n- APM samples periodically, may miss slow retention leaks\n- Leaks that grow <1MB/hour are hard to detect\n- **Solution**: Long-term monitoring (hours/days), use objgraph for deep analysis",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 6: Reality Check\n\n### What This DOESN'T Do\n\n1. **Replace code optimization**: APM shows you the problem, you still have to fix it\n2. **Eliminate load testing**: Profiling shows behavior under load, not capacity limits\n3. **Diagnose network issues**: Use distributed tracing (M7.1) for cross-service problems\n\n### Trade-offs You Accepted\n\n1. **Performance overhead**: 2-5% CPU even with conservative sampling (1% profiling, 10% traces)\n2. **Cost**: $51-100/month minimum, rising to $300+ at scale\n3. **Complexity**: 300+ lines of config code, requires profiling expertise\n4. **Data privacy**: Telemetry sent to Datadog (third-party service)\n\n### When This Approach Breaks\n\n**Scenario 1: APM Overhead Crushes Performance**\n- Production-safe config: 1% profiling, 10% sampling\n- If you increase to 10% profiling + 100% sampling \u2192 5-15% slowdown\n- **Solution**: Always use production-safe defaults, load test first\n\n**Scenario 2: Cost Explosion**\n- Expected: $51/month for small deployment\n- At 100K requests/hour: $300-500/month due to per-span fees ($5 per 1M spans)\n- **Solution**: Adaptive sampling (reduce rate at high traffic)\n\n**Scenario 3: Memory Leak Not Detected**\n- APM samples periodically, may miss slow retention leaks\n- Leaks that grow <1MB/hour are hard to detect\n- **Solution**: Long-term monitoring (hours/days), use objgraph for deep analysis",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Section 7: Alternative Solutions\n\n### Alternative 1: Open-Source APM (Grafana Tempo + Grafana)\n\n**Cost**: $0 (self-hosted) or $50/month (Grafana Cloud)\n\n**Pros:**\n- Full control over data\n- No vendor lock-in\n- Integrates with existing Grafana dashboards\n\n**Cons:**\n- Manual setup (2-3 days)\n- Less powerful profiling than Datadog\n- No automatic code-level flame graphs\n\n**When to use**: Budget <$100/month, need data sovereignty, already using Grafana\n\n---\n\n### Alternative 2: Cloud Provider APM\n\n**Options:**\n- AWS X-Ray: $5 per 1M requests\n- GCP Cloud Profiler: Free for GCP users\n- Azure Application Insights: Pay-per-use\n\n**Pros:**\n- Native cloud integration\n- Simpler if already on AWS/GCP/Azure\n- Often cheaper at low scale\n\n**Cons:**\n- Vendor lock-in\n- Limited cross-cloud visibility\n- Less powerful than Datadog\n\n**When to use**: Single cloud deployment, already invested in cloud ecosystem\n\n---\n\n### Alternative 3: Manual Profiling (py-spy)\n\n**Cost**: $0\n\n**Pros:**\n- Zero overhead when not profiling\n- On-demand profiling\n- Great for one-time investigations\n\n**Cons:**\n- Manual process\n- No continuous monitoring\n- No correlation with traces\n\n**When to use**: Low traffic, occasional debugging, tight budget\n\n**Example:**\n```bash\n# Profile a running Python process\npy-spy record -o profile.svg --pid 12345\n\n# Profile for 60 seconds\npy-spy record -d 60 -o profile.svg -- python app.py\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Section 8: When NOT to Use APM\n\n### Scenario 1: Low Traffic (<1,000 requests/day)\n\n**Why NOT to use APM:**\n- Insufficient data for meaningful profiling patterns\n- Sampling 10% of 1,000 requests = 100 traces (not enough)\n- APM costs ($51/month) exceed infrastructure costs\n\n**What to use instead:**\n- py-spy for one-time profiling\n- Manual logging during development\n- Wait until traffic grows >1K requests/hour\n\n---\n\n### Scenario 2: Pre-Optimization (No Known Performance Problem)\n\n**Why NOT to use APM:**\n- Premature optimization wastes time\n- No baseline to compare against\n- APM overhead without benefit\n\n**What to do instead:**\n- Wait until P95 latency crosses threshold (e.g., 3s)\n- Use basic metrics (Prometheus) to identify issues first\n- Add APM when you have specific bottleneck to investigate\n\n**Decision rule:** Only add APM when you have a known performance problem that basic metrics can't diagnose\n\n---\n\n### Scenario 3: Tight Budget (<$100/month total infrastructure)\n\n**Why NOT to use APM:**\n- APM minimum: $51/month\n- At $100 total budget, APM is 50%+ of costs\n- Open-source alternatives available\n\n**What to use instead:**\n- Grafana Tempo (self-hosted, $0)\n- py-spy (manual profiling, $0)\n- Cloud provider APM if already on AWS/GCP (often cheaper)\n\n---\n\n### Scenario 4: Highly Sensitive Data (Healthcare, Finance, Government)\n\n**Why NOT to use APM:**\n- Telemetry sent to third-party (Datadog)\n- May violate data sovereignty requirements\n- Compliance concerns (HIPAA, PCI-DSS)\n\n**What to use instead:**\n- Self-hosted APM (Grafana Tempo)\n- On-premise profiling tools\n- Cloud provider APM in same region/jurisdiction\n\n---\n\n### Summary: Use APM When You Have ALL of These\n\n```\n\u2713 High traffic (>1K requests/hour)\n\u2713 Known performance problems (P95 >3s)\n\u2713 Adequate budget ($50-200/month)\n\u2713 Data privacy clearance for third-party telemetry\n```\n\nIf missing ANY of the above, consider alternatives.\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Section 9: Common Failures\n\n### Failure 1: APM Overhead Crushing Performance (5-15% slowdown)\n\n**How it happens:**\n```python\n# \u274c WRONG - Too aggressive\nDD_PROFILING_CAPTURE_PCT = 10  # 10% profiling\nDD_TRACE_SAMPLE_RATE = 1.0      # 100% sampling\n```\n\n**Symptom:**\n- P95 latency increased from 800ms to 1.2s (50% slowdown)\n- CPU usage increased from 70% to 95%\n\n**The fix:**\n```python\n# \u2705 CORRECT - Production-safe\nDD_PROFILING_CAPTURE_PCT = 1   # 1% profiling\nDD_TRACE_SAMPLE_RATE = 0.1      # 10% sampling\nDD_PROFILING_MAX_TIME_USAGE_PCT = 5  # Safety limit\n```\n\n---\n\n### Failure 2: Profiling Crashes Application (OOM)\n\n**How it happens:**\n```python\n# \u274c WRONG - memory_profiler in production\nfrom memory_profiler import profile\n\n@profile  # Line-by-line tracking = huge overhead\ndef process_batch(docs):\n    # 10,000 docs \u00d7 2MB snapshot = 20GB memory\n```\n\n**The fix:**\n```python\n# \u2705 CORRECT - Use Datadog's sampling-based profiling\nDD_PROFILING_MEMORY_ENABLED = True\n# No decorator needed - automatic sampling\n```\n\n---\n\n### Failure 3: Memory Leak Detection Challenges\n\n**Why leaks are hard to detect:**\n- Slow retention leaks (<1MB/hour)\n- APM samples periodically, may miss gradual growth\n- Need long-term monitoring (hours/days)\n\n**Solution:**\n```python\n# Run long-term monitoring\nresults = monitor_memory_leak(iterations=100)\n\n# Use objgraph for deep analysis\nimport objgraph\nobjgraph.show_growth()  # Shows growing object types\n```\n\n---\n\n### Failure 4: Query Optimization Complexity\n\n**Challenge:**\n- EXPLAIN ANALYZE shows sequential scan\n- Adding index doesn't help\n- Query planner decisions are complex\n\n**Solution:**\n- Start simple, optimize incrementally\n- Test indexes in staging with production data volumes\n- Use APM as starting point, not final answer\n\n---\n\n### Failure 5: APM Cost Explosion ($500+ bill)\n\n**How it happens:**\n- Expected: $51/month\n- Actual: $523/month\n- Cause: High traffic + 100% sampling = 100M spans/month\n\n**The fix:**\n```python\n# Adaptive sampling based on traffic\nif requests_per_hour > 10000:\n    DD_TRACE_SAMPLE_RATE = 0.01  # 1% for high traffic\nelse:\n    DD_TRACE_SAMPLE_RATE = 0.1   # 10% for normal traffic\n\n# Monitor span count in Datadog billing dashboard\n# Set budget alerts at $100, $200 thresholds\n```\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Section 10: Decision Card\n\n### \u2705 BENEFIT\n\nDeep code-level profiling reveals bottlenecks down to specific function calls and line numbers. Reduces debugging time from hours to minutes by showing CPU hotspots, memory leaks, and slow queries with flame graphs. Correlates performance issues with traces from M7.1.\n\n---\n\n### \u274c LIMITATION\n\nAdds 2-5% CPU overhead in production even with conservative sampling (1% profiling, 10% trace sampling). Cost scales rapidly: $51/month minimum, rising to $300+/month at 100K requests/hour due to per-span analysis fees ($5 per 1M spans). Memory profiling shows allocations but struggles to detect slow retention-based leaks.\n\n---\n\n### \ud83d\udcb0 COST\n\n- **Time to implement**: 2-4 hours for initial setup, 1-2 days for production tuning\n- **Monthly cost**: $51-100 for small deployments (1-3 hosts), $300-800 for medium scale (10-15 hosts, 10M spans/day)\n- **Complexity**: 300+ lines of APM config code, requires understanding of profiling overhead vs visibility trade-offs\n\n---\n\n### \ud83e\udd14 USE WHEN\n\n- Traffic exceeds 1K requests/hour with known performance problems (P95 >3s)\n- Budget allows $50-200/month for APM\n- Team of 3+ engineers who will actively monitor dashboards\n- No compliance restrictions on sending telemetry to third-party services (Datadog)\n\n---\n\n### \ud83d\udeab AVOID WHEN\n\n- Traffic below 1K requests/hour (insufficient data for profiling patterns - use py-spy instead)\n- Budget under $100/month total (APM would be 50%+ of costs - use open-source Grafana Tempo)\n- Processing sensitive data requiring full data sovereignty (use self-hosted APM)\n- No known performance issues yet (premature optimization - wait until P95 crosses 3s)\n\n---\n\n### Decision Framework\n\n```\nChoose APM when you have ALL of:\n\u2713 High traffic (>1K requests/hour)\n\u2713 Known performance problems\n\u2713 Adequate budget ($50-200/month)\n\u2713 Data privacy clearance\n```\n\n**Save this card** - you'll reference it when deciding between Datadog APM, open-source alternatives, or manual profiling approaches.\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Section 11: Summary & Next Steps\n\n### What You Built Today\n\n- Full Datadog APM integration with OpenTelemetry bridge (connecting your M7.1 tracing)\n- Continuous profiling setup profiling 1% of requests with <5% CPU overhead\n- Memory leak detection system using tracemalloc\n- Production-safe configuration with cost controls\n\n### What You Learned\n\n\u2705 How APM complements tracing by showing function-level bottlenecks (not just span-level)  \n\u2705 Safe production profiling configuration (1% capture, 10% sampling, 5% max CPU)  \n\u2705 Memory leak detection patterns and tools  \n\u2705 When APM is overkill (low traffic, tight budgets, no known problems)  \n\u2705 Cost management strategies (adaptive sampling, span filtering)\n\n### Key Takeaways\n\n1. **APM is not a replacement for optimization** - it shows you the problem, you fix it\n2. **Start conservative** - 1% profiling, 10% sampling, increase only if safe\n3. **Monitor costs** - APM can get expensive at scale ($300+/month)\n4. **Know when NOT to use it** - low traffic, tight budgets, premature optimization\n\n### Production Checklist\n\nBefore deploying APM to production:\n\n- [ ] API keys configured in `.env`\n- [ ] Sampling rates production-safe (\u226410% traces, \u22641% profiling)\n- [ ] Safety limits set (`DD_PROFILING_MAX_TIME_USAGE_PCT=5`)\n- [ ] Load tested with APM enabled\n- [ ] APM overhead measured (<5% CPU increase)\n- [ ] Cost monitoring dashboard created\n- [ ] Budget alerts configured ($100, $200 thresholds)\n- [ ] Rollback plan documented\n\n### Next Steps\n\n1. **Explore Datadog UI**: If configured, visit https://app.datadoghq.com/apm/traces\n2. **Run load tests**: Generate traffic and observe APM profiling in real-time\n3. **Optimize bottlenecks**: Use APM to find and fix slow code paths\n4. **Next module**: Module 7.3 - Error Tracking & Root Cause Analysis\n\n---\n\n**Congratulations!** You've mastered Application Performance Monitoring for RAG systems.\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Final Demo: Complete APM Workflow\nprint(\"=\" * 60)\nprint(\"Module 7.2: Application Performance Monitoring - Complete Demo\")\nprint(\"=\" * 60)\nprint()\n\n# 1. Check APM status\nprint(\"1. APM Status:\")\nprint(f\"   Initialized: {apm_manager.is_initialized}\")\nprint(f\"   Configured: {apm_config.is_configured}\")\nprint()\n\n# 2. Run sample query\nprint(\"2. Processing sample query...\")\nquery = \"What are the compliance requirements?\"\nresult = pipeline.process_query(query, \"demo_user\")\nprint(f\"   \u2705 Query processed successfully\")\nprint()\n\n# 3. Check memory\nfrom src.l3_m7_application_performance_monitoring import MemoryProfiledComponent\nprofiler = MemoryProfiledComponent()\nstats = profiler.get_memory_stats()\nprint(\"3. Memory Statistics:\")\nprint(f\"   Current: {stats['current_mb']:.2f} MB\")\nprint(f\"   Peak: {stats['peak_mb']:.2f} MB\")\nprint(f\"   Growth: {stats['growth_mb']:.2f} MB\")\nprint()\n\n# 4. Summary\nprint(\"=\" * 60)\nprint(\"Demo Complete!\")\nprint()\nprint(\"Next steps:\")\nprint(\"- View traces in Datadog UI (if configured)\")\nprint(\"- Run load tests to generate profiling data\")\nprint(\"- Explore APM flame graphs and bottleneck analysis\")\nprint(\"=\" * 60)\n\n# Expected: All components work, APM captures profiling data if configured",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}