{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Learning Arc: Advanced Reranking Strategies\n\n## Purpose\nThis notebook teaches you to implement four complementary reranking strategies that improve search result quality beyond basic cross-encoder models. You'll learn when to apply ensemble voting, diversity algorithms (MMR), temporal boosting, and user personalization to production RAG systems while respecting performance budgets.\n\n## Concepts Covered\n- **Ensemble Reranking**: Combining multiple cross-encoder models using weighted averaging, Borda voting, or confidence fusion to reduce bias and improve accuracy by 8-12%\n- **Maximal Marginal Relevance (MMR)**: Balancing relevance versus diversity through iterative document selection with the formula `λ×relevance - (1-λ)×max_similarity`\n- **Temporal Boosting**: Applying exponential decay to document scores based on age, with linguistic cue detection for time-sensitive queries\n- **User Preference Learning**: Extracting features (source, type, depth, length) from documents and predicting user-specific appeal scores\n- **Pipeline Orchestration**: Combining all strategies while staying under 200ms P95 latency SLA\n\n## After Completing\nYou will be able to design production reranking pipelines that handle accuracy requirements (ensemble), diversity needs (MMR), time-sensitive queries (temporal), and personalization. You'll understand when NOT to use advanced reranking (first-pass retrieval <60% precision, low traffic, simple queries) and how to debug common failure modes like ensemble overconfidence, MMR over-diversification, recency bias overwhelming relevance, preference overfitting, and latency bottlenecks.\n\n## Context in Track\nThis is **Module 9.4** in the Level 3 advanced retrieval track, building on M9.1-M9.3 (Query Decomposition, Multi-Hop Retrieval, HyDE) and Level 1 M1.4 (basic cross-encoder reranking). It prepares you for M10 (Production Monitoring), M11 (Cost Optimization), and M12 (Multi-Modal RAG). The module assumes you have a working single-model reranking system and understand the limitations of optimizing purely for similarity without considering recency, diversity, or user preferences.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9.4: Advanced Reranking Strategies\n",
    "\n",
    "## Overview\n",
    "\n",
    "This educational module teaches Level 3 learners to implement advanced retrieval reranking beyond basic single cross-encoder models. This notebook covers four complementary strategies for improving search result quality in production RAG systems:\n",
    "\n",
    "1. **Ensemble Reranking with Voting** - Multiple cross-encoder models reduce bias\n",
    "2. **Maximal Marginal Relevance (MMR)** - Balance relevance vs diversity\n",
    "3. **Temporal/Recency Boosting** - Time-aware scoring\n",
    "4. **User Preference Learning** - Personalization based on implicit feedback\n",
    "\n",
    "> **Key Insight**: \"A single cross-encoder makes one judgment call. It doesn't know about recency, optimizes for similarity not diversity, and has no idea what this particular user cares about.\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Setup and Imports\n\nFirst, we'll import the required modules and load sample data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Offline mode detection\nimport os\nOFFLINE = not bool(os.getenv(\"OPENAI_API_KEY\"))\nif OFFLINE:\n    print(\"⚠️ Offline mode: model/network calls will be skipped or use mock data.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Configuration and Data Loading\n\nWe import the reranking classes and load configuration from environment variables. The example data includes 8 documents about ML frameworks with timestamps, metadata (source, type, technical depth), and a user profile with 150 interactions for testing personalization.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import sys\nimport os\nimport json\nimport numpy as np\n\n# Add project root to path for imports\nnotebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\nproject_root = os.path.dirname(notebook_dir)\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\nfrom src.l3_m9_advanced_reranking.l3_m9_advanced_reranking_strategies import (\n    Document,\n    EnsembleReranker,\n    MMRReranker,\n    TemporalReranker,\n    PersonalizationReranker,\n    AdvancedReranker\n)\nfrom src.l3_m9_advanced_reranking.config import get_config\n\n# Load configuration\nconfig = get_config()\nprint(\"✓ Configuration loaded\")\n\n# Load example data\nexample_data_path = os.path.join(project_root, \"example_data.json\")\nwith open(example_data_path, \"r\") as f:\n    data = json.load(f)\n\nquery = data[\"query\"]\nraw_documents = data[\"documents\"]\nuser_profile = data[\"user_profile\"]\n\nprint(f\"✓ Loaded {len(raw_documents)} documents\")\nprint(f\"✓ Query: '{query}'\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Strategy 1: Ensemble Reranking with Voting\n\nMultiple cross-encoder models score documents independently, then aggregate results through weighted averaging, rank-based voting, or confidence fusion. This approach reduces individual model bias and improves accuracy by 8-12% (from 78% to 86-88% precision).\n\n**Implementation Architecture:**\n- `EnsembleReranker` class accepts three configured models with normalized weights\n- `_aggregate_scores()` method supports:\n  - **Weighted average**: Direct score combination\n  - **Voting**: Borda count ranking system\n  - **Confidence fusion**: Magnitude-based weighting\n\n**Performance Budget**: 200-400ms latency",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Convert to Document objects\ndocuments = [\n    Document(id=d[\"id\"], text=d[\"text\"], metadata=d[\"metadata\"], score=0.5)\n    for d in raw_documents\n]\n\n# Initialize ensemble reranker with single model (to avoid long load times)\n# ⚠️ Skipping API calls if models unavailable\ntry:\n    ensemble = EnsembleReranker(\n        model_names=[config.RERANKER_MODELS[0]],  # Use first model only\n        aggregation=\"weighted\"\n    )\n    \n    # Perform ensemble reranking\n    result = ensemble.rerank(query, documents[:], top_k=3)\n    \n    print(f\"Latency: {result.latency_ms:.2f}ms\")\n    print(f\"Top 3 documents (ensemble):\")\n    for i, doc in enumerate(result.documents, 1):\n        print(f\"  {i}. {doc.id}: {doc.score:.4f}\")\nexcept Exception as e:\n    print(f\"⚠️ Skipping ensemble (model unavailable): {e}\")\n    \n# Expected: Top 3 reranked documents with scores",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Strategy 2: Maximal Marginal Relevance (MMR)\n\nThis algorithm balances relevance against diversity using the formula:\n\n```\nscore = λ × relevance - (1-λ) × max_similarity_to_selected\n```\n\nDocuments are selected iteratively, ensuring results contain varied perspectives rather than redundant content.\n\n**Key Parameters:**\n- `λ = 1.0`: Pure relevance (no diversity)\n- `λ = 0.0`: Pure diversity (may sacrifice relevance)\n- `λ = 0.7`: Recommended balance\n\n**Performance Budget**: 10-20ms latency",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Assign relevance scores to documents\nfor doc in documents:\n    doc.score = np.random.uniform(0.6, 0.9)\n\n# Initialize MMR reranker\nmmr = MMRReranker(lambda_param=config.MMR_LAMBDA)\n\n# Perform MMR reranking\nresult = mmr.rerank(documents[:], top_k=3)\n\nprint(f\"Latency: {result.latency_ms:.2f}ms\")\nprint(f\"Lambda (relevance/diversity balance): {config.MMR_LAMBDA}\")\nprint(f\"\\nTop 3 diverse documents (MMR):\")\nfor i, doc in enumerate(result.documents, 1):\n    print(f\"  {i}. {doc.id} - {doc.text[:60]}...\")\n\n# Expected: Top 3 diverse documents selected iteratively",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Strategy 3: Temporal/Recency Boosting\n\nTime-sensitive queries receive exponential decay penalties for older documents. Detection uses linguistic cues (\"latest,\" \"current,\" \"recent\") and applies multiplicative boosters to base relevance scores when appropriate.\n\n**Formula:**\n```\nrecency_multiplier = boost_factor × exp(-decay_rate × age_days)\nwhere decay_rate = ln(2) / decay_days (half-life)\n```\n\n**Key Parameters:**\n- `decay_days`: Half-life for exponential decay (default: 30 days)\n- `boost_factor`: Maximum boost for recent documents (default: 1.5)\n\n**Performance Budget**: 5ms latency",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Reset document scores\nfor doc in documents:\n    doc.score = 0.7\n\n# Initialize temporal reranker\ntemporal = TemporalReranker(\n    decay_days=config.RECENCY_DECAY_DAYS,\n    boost_factor=config.RECENCY_BOOST_FACTOR\n)\n\n# Check if query is temporal\nis_temporal = temporal.is_temporal_query(query)\nprint(f\"Query detected as temporal: {is_temporal}\")\nprint(f\"Query: '{query}'\")\n\n# Perform temporal reranking\nresult = temporal.rerank(query, documents[:])\n\nprint(f\"\\nLatency: {result.latency_ms:.2f}ms\")\nprint(f\"Top 3 documents (with recency boost):\")\nfor i, doc in enumerate(result.documents[:3], 1):\n    timestamp = doc.metadata.get(\"timestamp\", \"N/A\")\n    print(f\"  {i}. {doc.id}: {doc.score:.4f} (date: {timestamp[:10]})\")\n\n# Expected: Recent documents boosted in ranking",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Strategy 4: User Preference Learning\n\nClick-based implicit feedback trains lightweight models predicting document appeal per user. Features include document type, source, length, and technical depth, generating personalized score multipliers.\n\n**Feature Extraction:**\n- Document type match (tutorial, research, reference, opinion)\n- Technical depth alignment\n- Length preferences\n- Source preferences (blog, documentation, research papers)\n\n**Key Parameters:**\n- `min_interactions`: Minimum user interactions required (default: 100)\n\n**Performance Budget**: 15-30ms latency",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Reset document scores\nfor doc in documents:\n    doc.score = 0.7\n\n# Initialize personalization reranker\npersonalization = PersonalizationReranker(min_interactions=config.MIN_USER_INTERACTIONS)\n\n# Display user profile\nprint(f\"User: {user_profile['user_id']}\")\nprint(f\"Interactions: {user_profile['interaction_count']}\")\nprint(f\"Preferred sources: {user_profile['preferences']['preferred_sources']}\")\n\n# Perform personalization reranking\nresult = personalization.rerank(documents[:], user_profile)\n\nprint(f\"\\nLatency: {result.latency_ms:.2f}ms\")\nprint(f\"Personalized: {result.debug_info['personalized']}\")\nprint(f\"\\nTop 3 personalized documents:\")\nfor i, doc in enumerate(result.documents[:3], 1):\n    source = doc.metadata.get(\"source\", \"N/A\")\n    doc_type = doc.metadata.get(\"doc_type\", \"N/A\")\n    print(f\"  {i}. {doc.id}: {doc.score:.4f} (source: {source}, type: {doc_type})\")\n\n# Expected: Documents matching user preferences ranked higher",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Combined Pipeline: All Strategies Together\n\nThe `AdvancedReranker` orchestrates all four strategies with performance budgets. Combined approaches must stay under 200ms P95 response time for SLA compliance.\n\n**Pipeline Order:**\n1. Ensemble Reranking (200-400ms) - Initial scoring\n2. Temporal Boosting (5ms) - Time-aware adjustment\n3. Personalization (15-30ms) - User-specific weighting\n4. MMR Diversity (10-20ms) - Final selection\n\n**Total Budget**: <200ms P95 (may require disabling ensemble for speed)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Reset documents\ndocuments = [\n    Document(id=d[\"id\"], text=d[\"text\"], metadata=d[\"metadata\"], score=0.5)\n    for d in raw_documents\n]\n\n# Initialize advanced reranker (disable ensemble for speed)\nadvanced = AdvancedReranker(\n    enable_ensemble=False,  # Disabled to meet latency budget\n    enable_mmr=True,\n    enable_temporal=True,\n    enable_personalization=True,\n    config={\n        \"mmr_lambda\": config.MMR_LAMBDA,\n        \"decay_days\": config.RECENCY_DECAY_DAYS,\n        \"boost_factor\": config.RECENCY_BOOST_FACTOR,\n        \"min_interactions\": config.MIN_USER_INTERACTIONS\n    }\n)\n\n# Run full pipeline\nresult = advanced.rerank(query, documents, user_profile=user_profile, top_k=3)\n\nprint(f\"Total Pipeline Latency: {result.latency_ms:.2f}ms\")\nprint(f\"\\nPipeline Steps:\")\nfor step in result.debug_info[\"pipeline_steps\"]:\n    print(f\"  - {step['strategy']}: {step['latency_ms']:.2f}ms\")\n\nprint(f\"\\nFinal Top 3 Results:\")\nfor i, doc in enumerate(result.documents, 1):\n    print(f\"  {i}. {doc.id}: {doc.score:.4f}\")\n\n# Expected: All strategies applied, total latency <200ms",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Common Failure Modes\n\nUnderstanding when advanced reranking breaks is crucial for production deployments.\n\n### 1. Ensemble Overconfidence\n**When**: Multiple models agreeing on incorrect rankings when all trained similarly  \n**Fix**: Use diverse model architectures, switch to voting aggregation\n\n### 2. MMR Trade-offs\n**When**: Excessive diversity penalties sacrificing relevance  \n**Fix**: Increase lambda parameter (e.g., from 0.5 to 0.8)\n\n### 3. Recency Bias Overwhelming\n**When**: Time-sensitive boost drowning out actual relevance signals  \n**Fix**: Reduce boost_factor or increase decay_days\n\n### 4. Preference Learning Overfitting\n**When**: Models memorizing individual user quirks rather than generalizing  \n**Fix**: Increase min_interactions threshold, blend with base relevance more conservatively\n\n### 5. Latency Bottlenecks\n**When**: Three models running sequentially violates performance constraints  \n**Fix**: Disable ensemble or use single lightweight model",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Decision Card: When to Use Advanced Reranking\n\n### ✅ USE Advanced Reranking When:\n- First-pass retrieval has ≥60% precision\n- Query complexity requires multiple perspectives\n- User requests diverse results\n- Time-sensitive information matters\n- Personalization improves user engagement\n- Production traffic >1,000 queries/day\n- Accuracy improvements justify latency cost\n\n### ❌ DO NOT USE When:\n- **First-pass retrieval has <60% precision** (fix retrieval first!)\n- Simple, unambiguous queries\n- Low traffic (<1,000 queries/day)\n- Latency budget <100ms total\n- No user interaction data for personalization\n- Cost/complexity exceeds value\n\n### Strategy Selection Matrix\n\n| Strategy | When to Apply | Skip If |\n|----------|---------------|---------|\n| **Ensemble** | High-stakes queries, need confidence | Latency critical, simple queries |\n| **MMR** | User wants diverse perspectives | Single answer expected |\n| **Temporal** | Query contains temporal keywords | Evergreen content queries |\n| **Personalization** | User has ≥100 interactions | New user, no history |\n\n### Production Checklist\n1. ✓ Measure first-pass retrieval precision\n2. ✓ Profile latency budget per strategy\n3. ✓ Monitor P95 latency in production\n4. ✓ A/B test accuracy improvements\n5. ✓ Track user engagement metrics",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Summary and Next Steps\n\n### What You Learned\n✓ **Ensemble Reranking**: Combining multiple models reduces bias and improves accuracy by 8-12%  \n✓ **MMR Diversity**: Balancing relevance vs diversity for varied perspectives  \n✓ **Temporal Boosting**: Time-aware scoring with exponential decay  \n✓ **Personalization**: User preference learning from implicit feedback  \n✓ **Combined Pipeline**: Orchestrating all strategies with performance budgets\n\n### Key Takeaways\n1. **Fix retrieval first**: Advanced reranking only helps when first-pass retrieval has ≥60% precision\n2. **Mind the latency**: Combined pipeline must stay under 200ms P95 for SLA compliance\n3. **Choose strategies wisely**: Not all queries need all strategies—use the decision matrix\n4. **Monitor production**: Track latency, accuracy improvements, and user engagement\n5. **Handle failures gracefully**: Understand common failure modes and their fixes\n\n### Prerequisites for This Module\n- Level 1 M1.4: Basic Cross-Encoder Reranking\n- M9.1-M9.3: Query Decomposition, Multi-Hop Retrieval, HyDE\n- Working single-model reranking system\n\n### Next Modules\n- **M10**: Production Monitoring and Observability\n- **M11**: Cost Optimization and Caching Strategies\n- **M12**: Multi-Modal RAG Systems\n\n### Additional Resources\n- [Sentence Transformers Cross-Encoders](https://www.sbert.net/examples/applications/cross-encoder/README.html)\n- [MMR Original Paper](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf)\n- README.md for detailed documentation and troubleshooting",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}